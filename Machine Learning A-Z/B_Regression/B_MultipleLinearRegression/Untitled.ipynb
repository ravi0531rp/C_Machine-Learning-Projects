{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiple Linear Regression Theory\n",
    "Multiple Linear Regression\n",
    "Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to observed data. Every value of the independent variable x is associated with a value of the dependent variable y. The population regression line for p explanatory variables x1, x2, ... , xp is defined to be  y = 0 + 1x1 + 2x2 + ... +  pxp. This line describes how the mean response y changes with the explanatory variables. The observed values for y vary about their means y and are assumed to have the same standard deviation . The fitted values b0, b1, ..., bp estimate the parameters 0, 1, ..., p of the population regression line.\n",
    "Since the observed values for y vary about their means y, the multiple regression model includes a term for this variation. In words, the model is expressed as DATA = FIT + RESIDUAL, where the \"FIT\" term represents the expression 0 +  1x1 +  2x2 + ... pxp. The \"RESIDUAL\" term represents the deviations of the observed values y from their means y, which are normally distributed with mean 0 and variance . The notation for the model deviations is .\n",
    "\n",
    "Formally, the model for multiple linear regression, given n observations, is \n",
    "yi = 0 +  1xi1 +  2xi2 + ... pxip +  i for i = 1,2, ... n.\n",
    "\n",
    "In the least-squares model, the best-fitting line for the observed data is calculated by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is 0). Because the deviations are first squared, then summed, there are no cancellations between positive and negative values. The least-squares estimates b0, b1, ... bp are usually computed by statistical software.\n",
    "\n",
    "The values fit by the equation b0 + b1xi1 + ... + bpxip are denoted i, and the residuals ei are equal to yi -  i, the difference between the observed and fitted values. The sum of the residuals is equal to zero.\n",
    "\n",
    "The variance ² may be estimated by s² = , also known as the mean-squared error (or MSE). \n",
    "The estimate of the standard error s is the square root of the MSE.\n",
    "\n",
    "Example\n",
    "The dataset \"Healthy Breakfast\" contains, among other variables, the Consumer Reports ratings of 77 cereals and the number of grams of sugar contained in each serving. (Data source: Free publication available in many grocery stores. Dataset available through the Statlib Data and Story Library (DASL).)\n",
    "A simple linear regression model considering \"Sugars\" as the explanatory variable and \"Rating\" as the response variable produced the regression line \n",
    "Rating = 59.3 - 2.40 Sugars, with the square of the correlation r² = 0.577 (see Inference in Linear Regression for more details on this regression).\n",
    "\n",
    "The \"Healthy Breakfast\" dataset includes several other variables, including grams of fat per serving and grams of dietary fiber per serving. Is the model significantly improved when these variables are included?\n",
    "\n",
    "Suppose we are first interested in adding the \"Fat\" variable. The correlation between \"Fat\" and \"Rating\" is equal to -0.409, while the correlation between \"Sugars\" and \"Fat\" is equal to 0.271. Since \"Fat\" and \"Sugar\" are not highly correlated, the addition of the \"Fat\" variable may significantly improve the model.\n",
    "\n",
    "The MINITAB \"Regress\" command produced the following results:\n",
    "\n",
    "Regression Analysis\n",
    "\n",
    "The regression equation is\n",
    "Rating = 61.1 - 3.07 Fat - 2.21 Sugars\n",
    "\n",
    "\n",
    "After fitting the regression line, it is important to investigate the residuals to determine whether or not they appear to fit the assumption of a normal distribution. A normal quantile plot of the standardized residuals y -  is shown to the left. Despite two large values which may be outliers in the data, the residuals do not seem to deviate from a random sample from a normal distribution in any systematic manner.\n",
    "The MINITAB output provides a great deal of information. Under the equation for the regression line, the output provides the least-squares estimates for each parameter, listed in the \"Coef\" column next to the variable to which it corresponds. The calculated standard deviations are provided in the second column.\n",
    "\n",
    "Predictor       Coef       StDev          T        P\n",
    "Constant      61.089       1.953      31.28    0.000\n",
    "Fat           -3.066       1.036      -2.96    0.004\n",
    "Sugars       -2.2128      0.2347      -9.43    0.000\n",
    "\n",
    "S = 8.755       R-Sq = 62.2%     R-Sq(adj) = 61.2%\n",
    "Significance Tests\n",
    "The third column \"T\" of the MINITAB \"REGRESS\" output provides test statistics. As in linear regression, one wishes to test the significance of the parameters included. For any of the variables xj included in a multiple regression model, the null hypothesis states that the coefficient  j is equal to 0. The alternative hypothesis may be one-sided or two-sided, stating that j is either less than 0, greater than 0, or simply not equal to 0.\n",
    "The test statistic t is equal to bj/sbj, the parameter estimate divided by its standard deviation. This value follows a t(n-p-1) distribution when p variables are included in the model.\n",
    "\n",
    "In the example above, the parameter estimate for the \"Fat\" variable is -3.066 with standard deviation 1.036 The test statistic is t = -3.066/1.036 = -2.96, provided in the \"T\" column of the MINITAB output. For a two-sided test, the probability of interest is 2P(T>|-2.96|) for the t(77-2-1) = t(74) distribution, which is about 0.004. The \"P\" column of the MINITAB output provides the P-value associated with the two-sided test. Since the P-values for both \"Fat\" and \"Sugar\" are highly significant, both variables may be included in the model.\n",
    "\n",
    "Condidence Intervals for Regression Parameters\n",
    "A level C confidence interval for the parameter j may be computed from the estimate bj using the computed standard deviations and the appropriate critical value t* from the t(n-p-1) distribution. The confidence interval for  j takes the form bj + t*sbj.\n",
    "Continuing with the \"Healthy Breakfast\" example, suppose we choose to add the \"Fiber\" variable to our model. The MINITAB results are the following:\n",
    "\n",
    "Regression Analysis\n",
    "\n",
    "The regression equation is\n",
    "Rating = 53.4 - 3.48 Fat + 2.95 Fiber - 1.96 Sugars\n",
    "\n",
    "Predictor       Coef       StDev          T        P\n",
    "Constant      53.437       1.342      39.82    0.000\n",
    "Fat          -3.4802      0.6209      -5.61    0.000\n",
    "Fiber         2.9503      0.2549      11.57    0.000\n",
    "Sugars       -1.9640      0.1420     -13.83    0.000\n",
    "\n",
    "S = 5.235       R-Sq = 86.7%     R-Sq(adj) = 86.1%\n",
    "The squared multiple correlation R² is now equal to 0.861, and all of the variables are significant by the t tests. Examination of the residuals indicates no unusual patterns. The inclusion of the \"Fat,\" \"Fiber,\" and \"Sugars\" variables explains 86.7% of the variability of the data, a significant improvement over the smaller models.\n",
    "For additional tests and a continuation of this example, see ANOVA for Multiple Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assumptions of Linear Regression\n",
    "\n",
    "1) Linearity\n",
    "2) Homoscedasticity\n",
    "3) Multivariate Normality\n",
    "4) Independence of errors\n",
    "5) Lack of multi-collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   # Always omit one dummy variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods of model building\n",
    "\n",
    "1) All-in\n",
    "2) Backward Elimination\n",
    "3) Forward Selection\n",
    "4) Bidirectional Elimination\n",
    "5) Score Comparison\n",
    "\n",
    "\n",
    "** All in means throwing all the variables.Just by using good knowledge of the field.\n",
    "\n",
    "\n",
    "** Backward elimination :-\n",
    "        Step 1 :- Select a significance level to stay in the model(say, sl=0.5) ;\n",
    "        Step 2 :- Fit the model with all possible predictors ;\n",
    "        Step 3 :- Consider the predictor with the highest P-value. If P>sl, go to step 4. else your model is ready. ;\n",
    "        Step 4 :- Remove the Predictor ;\n",
    "        Step 5 :- Fit the model without this variable. ;\n",
    "        \n",
    "\n",
    "** Forward Selection :-\n",
    "        Reverse(Construct from one variable lin regression-> then add one more var to the model with least P val -> again look for least p -> add one more var)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.preprocessing import StandardScaler\\nsc_X = StandardScaler()\\nX_train = sc_X.fit_transform(X_train)\\nX_test = sc_X.transform(X_test)\\nsc_y = StandardScaler()\\ny_train = sc_y.fit_transform(y_train)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing Template\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "\n",
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[:, 3] = labelencoder_X.fit_transform(X[:, 3])  # the column we want to encode is 3\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])  # again the index of the column\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "\n",
    "# Avoiding the dummy variable trap\n",
    "X=X[:,1:]  # we exclude the first column, we do not have to do it here as the lib takes care of it, but sometimes it is reqd. \n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "sc_y = StandardScaler()\n",
    "y_train = sc_y.fit_transform(y_train)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>73721.615600</td>\n",
       "      <td>121344.639600</td>\n",
       "      <td>211025.097800</td>\n",
       "      <td>112012.639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>45902.256482</td>\n",
       "      <td>28017.802755</td>\n",
       "      <td>122290.310726</td>\n",
       "      <td>40306.180338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>51283.140000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14681.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39936.370000</td>\n",
       "      <td>103730.875000</td>\n",
       "      <td>129300.132500</td>\n",
       "      <td>90138.902500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>73051.080000</td>\n",
       "      <td>122699.795000</td>\n",
       "      <td>212716.240000</td>\n",
       "      <td>107978.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>101602.800000</td>\n",
       "      <td>144842.180000</td>\n",
       "      <td>299469.085000</td>\n",
       "      <td>139765.977500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>165349.200000</td>\n",
       "      <td>182645.560000</td>\n",
       "      <td>471784.100000</td>\n",
       "      <td>192261.830000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           R&D Spend  Administration  Marketing Spend         Profit\n",
       "count      50.000000       50.000000        50.000000      50.000000\n",
       "mean    73721.615600   121344.639600    211025.097800  112012.639200\n",
       "std     45902.256482    28017.802755    122290.310726   40306.180338\n",
       "min         0.000000    51283.140000         0.000000   14681.400000\n",
       "25%     39936.370000   103730.875000    129300.132500   90138.902500\n",
       "50%     73051.080000   122699.795000    212716.240000  107978.190000\n",
       "75%    101602.800000   144842.180000    299469.085000  139765.977500\n",
       "max    165349.200000   182645.560000    471784.100000  192261.830000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 1.0000000e+00, 1.6534920e+05, 1.3689780e+05,\n",
       "        4.7178410e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.6259770e+05, 1.5137759e+05,\n",
       "        4.4389853e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.5344151e+05, 1.0114555e+05,\n",
       "        4.0793454e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 1.4437241e+05, 1.1867185e+05,\n",
       "        3.8319962e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.4210734e+05, 9.1391770e+04,\n",
       "        3.6616842e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 1.3187690e+05, 9.9814710e+04,\n",
       "        3.6286136e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.3461546e+05, 1.4719887e+05,\n",
       "        1.2771682e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.3029813e+05, 1.4553006e+05,\n",
       "        3.2387668e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 1.2054252e+05, 1.4871895e+05,\n",
       "        3.1161329e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.2333488e+05, 1.0867917e+05,\n",
       "        3.0498162e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.0191308e+05, 1.1059411e+05,\n",
       "        2.2916095e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.0067196e+05, 9.1790610e+04,\n",
       "        2.4974455e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 9.3863750e+04, 1.2732038e+05,\n",
       "        2.4983944e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 9.1992390e+04, 1.3549507e+05,\n",
       "        2.5266493e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.1994324e+05, 1.5654742e+05,\n",
       "        2.5651292e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 1.1452361e+05, 1.2261684e+05,\n",
       "        2.6177623e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 7.8013110e+04, 1.2159755e+05,\n",
       "        2.6434606e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 9.4657160e+04, 1.4507758e+05,\n",
       "        2.8257431e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 9.1749160e+04, 1.1417579e+05,\n",
       "        2.9491957e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 8.6419700e+04, 1.5351411e+05,\n",
       "        0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 7.6253860e+04, 1.1386730e+05,\n",
       "        2.9866447e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 7.8389470e+04, 1.5377343e+05,\n",
       "        2.9973729e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 7.3994560e+04, 1.2278275e+05,\n",
       "        3.0331926e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 6.7532530e+04, 1.0575103e+05,\n",
       "        3.0476873e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 7.7044010e+04, 9.9281340e+04,\n",
       "        1.4057481e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 6.4664710e+04, 1.3955316e+05,\n",
       "        1.3796262e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 7.5328870e+04, 1.4413598e+05,\n",
       "        1.3405007e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 7.2107600e+04, 1.2786455e+05,\n",
       "        3.5318381e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 6.6051520e+04, 1.8264556e+05,\n",
       "        1.1814820e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 6.5605480e+04, 1.5303206e+05,\n",
       "        1.0713838e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 6.1994480e+04, 1.1564128e+05,\n",
       "        9.1131240e+04],\n",
       "       [0.0000000e+00, 1.0000000e+00, 6.1136380e+04, 1.5270192e+05,\n",
       "        8.8218230e+04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 6.3408860e+04, 1.2921961e+05,\n",
       "        4.6085250e+04],\n",
       "       [1.0000000e+00, 0.0000000e+00, 5.5493950e+04, 1.0305749e+05,\n",
       "        2.1463481e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 4.6426070e+04, 1.5769392e+05,\n",
       "        2.1079767e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 4.6014020e+04, 8.5047440e+04,\n",
       "        2.0551764e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 2.8663760e+04, 1.2705621e+05,\n",
       "        2.0112682e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 4.4069950e+04, 5.1283140e+04,\n",
       "        1.9702942e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 2.0229590e+04, 6.5947930e+04,\n",
       "        1.8526510e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 3.8558510e+04, 8.2982090e+04,\n",
       "        1.7499930e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 2.8754330e+04, 1.1854605e+05,\n",
       "        1.7279567e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 2.7892920e+04, 8.4710770e+04,\n",
       "        1.6447071e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 2.3640930e+04, 9.6189630e+04,\n",
       "        1.4800111e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 1.5505730e+04, 1.2738230e+05,\n",
       "        3.5534170e+04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 2.2177740e+04, 1.5480614e+05,\n",
       "        2.8334720e+04],\n",
       "       [0.0000000e+00, 1.0000000e+00, 1.0002300e+03, 1.2415304e+05,\n",
       "        1.9039300e+03],\n",
       "       [1.0000000e+00, 0.0000000e+00, 1.3154600e+03, 1.1581621e+05,\n",
       "        2.9711446e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.3542692e+05,\n",
       "        0.0000000e+00],\n",
       "       [0.0000000e+00, 1.0000000e+00, 5.4205000e+02, 5.1743150e+04,\n",
       "        0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.1698380e+05,\n",
       "        4.5173060e+04]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training on the Training set and  Predicting  on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building an Optimal Model using Backward Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodel is used to eval the P value. One more point :- the b0*x0 where x0 is 1 is not included in statsmodel , unlike scikit. Hence we need to take care of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.append(arr=np.ones((50,1)).astype(int),values=X,axis=1)  # add a column of ones as the first column of X\n",
    "# in other words, for getting ones in the col 1, we append x to an array of ones , so that the ones remain at column 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new matrix of features that will be optimal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_opt= X[:,[0,1,2,3,4,5]]                                                         # it will have only the highly impactful vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()       # Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x236e17406d8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now get the summary for the P values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   169.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 24 Aug 2018</td> <th>  Prob (F-statistic):</th> <td>1.34e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>03:07:04</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1074.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.013e+04</td> <td> 6884.820</td> <td>    7.281</td> <td> 0.000</td> <td> 3.62e+04</td> <td>  6.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  198.7888</td> <td> 3371.007</td> <td>    0.059</td> <td> 0.953</td> <td>-6595.030</td> <td> 6992.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -41.8870</td> <td> 3256.039</td> <td>   -0.013</td> <td> 0.990</td> <td>-6604.003</td> <td> 6520.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.369</td> <td> 0.000</td> <td>    0.712</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.517</td> <td> 0.608</td> <td>   -0.132</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.574</td> <td> 0.123</td> <td>   -0.008</td> <td>    0.062</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.782</td> <th>  Durbin-Watson:     </th> <td>   1.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.41e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.572</td> <th>  Cond. No.          </th> <td>1.45e+06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.45e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     169.9\n",
       "Date:                Fri, 24 Aug 2018   Prob (F-statistic):           1.34e-27\n",
       "Time:                        03:07:04   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1063.\n",
       "Df Residuals:                      44   BIC:                             1074.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n",
       "x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n",
       "x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n",
       "x3             0.8060      0.046     17.369      0.000       0.712       0.900\n",
       "x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n",
       "x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n",
       "==============================================================================\n",
       "Omnibus:                       14.782   Durbin-Watson:                   1.283\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n",
       "Skew:                          -0.948   Prob(JB):                     2.41e-05\n",
       "Kurtosis:                       5.572   Cond. No.                     1.45e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.45e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_OLS.summary()   # the lower the P value, the more is the significance of contribution of the var in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_opt= X[:,[0,1,2,3,5]]  \n",
    "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   215.8</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 24 Aug 2018</td> <th>  Prob (F-statistic):</th> <td>9.72e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>03:07:04</td>     <th>  Log-Likelihood:    </th> <td> -525.53</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1061.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1071.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 4.696e+04</td> <td> 3119.471</td> <td>   15.053</td> <td> 0.000</td> <td> 4.07e+04</td> <td> 5.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  140.7869</td> <td> 3341.599</td> <td>    0.042</td> <td> 0.967</td> <td>-6589.538</td> <td> 6871.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -19.5234</td> <td> 3229.138</td> <td>   -0.006</td> <td> 0.995</td> <td>-6523.340</td> <td> 6484.294</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.7967</td> <td>    0.042</td> <td>   18.771</td> <td> 0.000</td> <td>    0.711</td> <td>    0.882</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0298</td> <td>    0.016</td> <td>    1.842</td> <td> 0.072</td> <td>   -0.003</td> <td>    0.062</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.640</td> <th>  Durbin-Watson:     </th> <td>   1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.938</td> <th>  Prob(JB):          </th> <td>2.70e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.565</td> <th>  Cond. No.          </th> <td>8.56e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.56e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.950\n",
       "Model:                            OLS   Adj. R-squared:                  0.946\n",
       "Method:                 Least Squares   F-statistic:                     215.8\n",
       "Date:                Fri, 24 Aug 2018   Prob (F-statistic):           9.72e-29\n",
       "Time:                        03:07:04   Log-Likelihood:                -525.53\n",
       "No. Observations:                  50   AIC:                             1061.\n",
       "Df Residuals:                      45   BIC:                             1071.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       4.696e+04   3119.471     15.053      0.000    4.07e+04    5.32e+04\n",
       "x1           140.7869   3341.599      0.042      0.967   -6589.538    6871.112\n",
       "x2           -19.5234   3229.138     -0.006      0.995   -6523.340    6484.294\n",
       "x3             0.7967      0.042     18.771      0.000       0.711       0.882\n",
       "x4             0.0298      0.016      1.842      0.072      -0.003       0.062\n",
       "==============================================================================\n",
       "Omnibus:                       14.640   Durbin-Watson:                   1.257\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.037\n",
       "Skew:                          -0.938   Prob(JB):                     2.70e-05\n",
       "Kurtosis:                       5.565   Cond. No.                     8.56e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.56e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_OLS.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_opt= X[:,[0,1,2,5]]  \n",
    "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   19.71</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 24 Aug 2018</td> <th>  Prob (F-statistic):</th> <td>2.32e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>03:07:04</td>     <th>  Log-Likelihood:    </th> <td> -579.99</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1168.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    46</td>      <th>  BIC:               </th> <td>   1176.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.864e+04</td> <td> 8984.018</td> <td>    6.527</td> <td> 0.000</td> <td> 4.06e+04</td> <td> 7.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-1194.5800</td> <td> 9818.999</td> <td>   -0.122</td> <td> 0.904</td> <td> -2.1e+04</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 4196.5465</td> <td> 9467.707</td> <td>    0.443</td> <td> 0.660</td> <td>-1.49e+04</td> <td> 2.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.2480</td> <td>    0.033</td> <td>    7.525</td> <td> 0.000</td> <td>    0.182</td> <td>    0.314</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.720</td> <th>  Durbin-Watson:     </th> <td>   1.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.156</td> <th>  Jarque-Bera (JB):  </th> <td>   2.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.299</td> <th>  Prob(JB):          </th> <td>   0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.034</td> <th>  Cond. No.          </th> <td>8.11e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.11e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.562\n",
       "Model:                            OLS   Adj. R-squared:                  0.534\n",
       "Method:                 Least Squares   F-statistic:                     19.71\n",
       "Date:                Fri, 24 Aug 2018   Prob (F-statistic):           2.32e-08\n",
       "Time:                        03:07:04   Log-Likelihood:                -579.99\n",
       "No. Observations:                  50   AIC:                             1168.\n",
       "Df Residuals:                      46   BIC:                             1176.\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.864e+04   8984.018      6.527      0.000    4.06e+04    7.67e+04\n",
       "x1         -1194.5800   9818.999     -0.122      0.904    -2.1e+04    1.86e+04\n",
       "x2          4196.5465   9467.707      0.443      0.660   -1.49e+04    2.33e+04\n",
       "x3             0.2480      0.033      7.525      0.000       0.182       0.314\n",
       "==============================================================================\n",
       "Omnibus:                        3.720   Durbin-Watson:                   1.174\n",
       "Prob(Omnibus):                  0.156   Jarque-Bera (JB):                2.973\n",
       "Skew:                          -0.299   Prob(JB):                        0.226\n",
       "Kurtosis:                       4.034   Cond. No.                     8.11e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.11e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_OLS.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_opt= X[:,[0,1,2]]  \n",
    "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>  -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>  0.5748</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 24 Aug 2018</td> <th>  Prob (F-statistic):</th>  <td> 0.567</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>03:07:05</td>     <th>  Log-Likelihood:    </th> <td> -600.05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1206.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    47</td>      <th>  BIC:               </th> <td>   1212.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 1.039e+05</td> <td> 9861.636</td> <td>   10.536</td> <td> 0.000</td> <td> 8.41e+04</td> <td> 1.24e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> 1.487e+04</td> <td> 1.42e+04</td> <td>    1.050</td> <td> 0.299</td> <td>-1.36e+04</td> <td> 4.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 9851.2712</td> <td> 1.39e+04</td> <td>    0.706</td> <td> 0.483</td> <td>-1.82e+04</td> <td> 3.79e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.111</td> <th>  Durbin-Watson:     </th> <td>   0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.946</td> <th>  Jarque-Bera (JB):  </th> <td>   0.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.104</td> <th>  Prob(JB):          </th> <td>   0.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.762</td> <th>  Cond. No.          </th> <td>    3.70</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.024\n",
       "Model:                            OLS   Adj. R-squared:                 -0.018\n",
       "Method:                 Least Squares   F-statistic:                    0.5748\n",
       "Date:                Fri, 24 Aug 2018   Prob (F-statistic):              0.567\n",
       "Time:                        03:07:05   Log-Likelihood:                -600.05\n",
       "No. Observations:                  50   AIC:                             1206.\n",
       "Df Residuals:                      47   BIC:                             1212.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1.039e+05   9861.636     10.536      0.000    8.41e+04    1.24e+05\n",
       "x1          1.487e+04   1.42e+04      1.050      0.299   -1.36e+04    4.34e+04\n",
       "x2          9851.2712   1.39e+04      0.706      0.483   -1.82e+04    3.79e+04\n",
       "==============================================================================\n",
       "Omnibus:                        0.111   Durbin-Watson:                   0.081\n",
       "Prob(Omnibus):                  0.946   Jarque-Bera (JB):                0.207\n",
       "Skew:                           0.104   Prob(JB):                        0.902\n",
       "Kurtosis:                       2.762   Cond. No.                         3.70\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_OLS.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
